{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:32:28.243099Z","iopub.execute_input":"2022-04-28T23:32:28.244099Z","iopub.status.idle":"2022-04-28T23:32:29.089734Z","shell.execute_reply.started":"2022-04-28T23:32:28.243978Z","shell.execute_reply":"2022-04-28T23:32:29.088904Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Thu Apr 28 23:32:28 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   44C    P0    41W / 250W |  15999MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport IPython","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:14.742172Z","iopub.execute_input":"2022-04-28T23:31:14.742732Z","iopub.status.idle":"2022-04-28T23:31:19.94141Z","shell.execute_reply.started":"2022-04-28T23:31:14.74269Z","shell.execute_reply":"2022-04-28T23:31:19.940642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/flickr-image-dataset/flickr30k_images'\nimage_dir = f'{data_dir}/flickr30k_images'\ncsv_file = f'{data_dir}/results.csv'\n\ndf = pd.read_csv(csv_file, delimiter='|')\n\nprint(f'[INFO] The shape of dataframe: {df.shape}')\nprint(f'[INFO] The columns in the dataframe: {df.columns}')\nprint(f'[INFO] Unique image names: {len(pd.unique(df[\"image_name\"]))}')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:19.942821Z","iopub.execute_input":"2022-04-28T23:31:19.943077Z","iopub.status.idle":"2022-04-28T23:31:20.309234Z","shell.execute_reply.started":"2022-04-28T23:31:19.943043Z","shell.execute_reply":"2022-04-28T23:31:20.308503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A quick observation here is to see that the dataframe has `158915` elements but only `31783` image names. This means that there is a duplicacy involved. On further inspection we will see that each image has 5 unique captions attached to it ($31783\\times 5=158915$)\n\nWhile looking into the dataframe I found out that `19999` had some messed up entries. This has led me to manually change the entries in that row.","metadata":{}},{"cell_type":"code","source":"df.columns = ['image_name', 'comment_number', 'comment']\ndel df['comment_number']\n\n# Under scrutiny I had found that 19999 had a messed up entry\ndf['comment'][19999] = ' A dog runs across the grass .'\n\n# Image names now correspond to the absolute position\ndf['image_name'] = image_dir+'/'+df['image_name']\n\n# <start> comment <end>\ndf['comment'] = \"<start> \"+df['comment']+\" <end>\"","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:20.311278Z","iopub.execute_input":"2022-04-28T23:31:20.311531Z","iopub.status.idle":"2022-04-28T23:31:20.411054Z","shell.execute_reply.started":"2022-04-28T23:31:20.311494Z","shell.execute_reply":"2022-04-28T23:31:20.410284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\nSIZE = len(df)\n\ntrain_size = int(0.7* SIZE) \nval_size = int(0.1* SIZE)\ntest_size = int(0.2* SIZE)\n\ntrain_size, val_size, test_size\n\ntrain_df = df.iloc[:train_size,:]\nval_df = df.iloc[train_size+1:train_size+val_size,:]\ntest_df = df.iloc[train_size+val_size+1:,:]","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:20.41246Z","iopub.execute_input":"2022-04-28T23:31:20.412727Z","iopub.status.idle":"2022-04-28T23:31:20.469615Z","shell.execute_reply.started":"2022-04-28T23:31:20.412691Z","shell.execute_reply":"2022-04-28T23:31:20.468876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the dataframe accordingly","metadata":{}},{"cell_type":"code","source":"# Enter different indices.\nindex = 200\n\nimage_name = train_df['image_name'][index]\ncomment = train_df['comment'][index]\n\nprint(comment)\n\nIPython.display.Image(filename=image_name)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:20.470983Z","iopub.execute_input":"2022-04-28T23:31:20.471245Z","iopub.status.idle":"2022-04-28T23:31:20.498051Z","shell.execute_reply.started":"2022-04-28T23:31:20.471211Z","shell.execute_reply":"2022-04-28T23:31:20.497385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose the top 5000 words from the vocabulary\ntop_k = 10000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n\n# build the vocabulary\ntokenizer.fit_on_texts(train_df['comment'])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:20.498908Z","iopub.execute_input":"2022-04-28T23:31:20.499106Z","iopub.status.idle":"2022-04-28T23:31:22.379211Z","shell.execute_reply.started":"2022-04-28T23:31:20.49908Z","shell.execute_reply":"2022-04-28T23:31:22.378407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a sanity check function\ndef check_vocab(word):\n    i = tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {tokenizer.index_word[i]}\")\n    \ncheck_vocab(\"pajama\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:22.380887Z","iopub.execute_input":"2022-04-28T23:31:22.38138Z","iopub.status.idle":"2022-04-28T23:31:22.387502Z","shell.execute_reply.started":"2022-04-28T23:31:22.381342Z","shell.execute_reply":"2022-04-28T23:31:22.386608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are padding the sentences so that each of the sentences are of the same length.","metadata":{}},{"cell_type":"code","source":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\n# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_df['comment'])\nval_seqs = tokenizer.texts_to_sequences(val_df['comment'])\ntest_seqs = tokenizer.texts_to_sequences(test_df['comment'])\n\n# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ntrain_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\nval_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')\ntest_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:22.389043Z","iopub.execute_input":"2022-04-28T23:31:22.389323Z","iopub.status.idle":"2022-04-28T23:31:25.465395Z","shell.execute_reply.started":"2022-04-28T23:31:22.389286Z","shell.execute_reply":"2022-04-28T23:31:25.464609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cap_ds = tf.data.Dataset.from_tensor_slices(train_cap_vector)\nval_cap_ds = tf.data.Dataset.from_tensor_slices(val_cap_vector)\ntest_cap_ds = tf.data.Dataset.from_tensor_slices(test_cap_vector)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:25.468464Z","iopub.execute_input":"2022-04-28T23:31:25.468724Z","iopub.status.idle":"2022-04-28T23:31:28.258864Z","shell.execute_reply.started":"2022-04-28T23:31:25.468688Z","shell.execute_reply":"2022-04-28T23:31:28.258169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef load_img(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, (224, 224))\n    return img\n\ntrain_img_name = train_df['image_name'].values\nval_img_name = val_df['image_name'].values\ntest_img_name = test_df['image_name'].values\n\ntrain_img_ds = tf.data.Dataset.from_tensor_slices(train_img_name).map(load_img)\nval_img_ds = tf.data.Dataset.from_tensor_slices(val_img_name).map(load_img)\ntest_img_ds = tf.data.Dataset.from_tensor_slices(test_img_name).map(load_img)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:28.260179Z","iopub.execute_input":"2022-04-28T23:31:28.260432Z","iopub.status.idle":"2022-04-28T23:31:28.402923Z","shell.execute_reply.started":"2022-04-28T23:31:28.260401Z","shell.execute_reply":"2022-04-28T23:31:28.402246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Joint data","metadata":{}},{"cell_type":"code","source":"# prefecth and batch the dataset\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 512\n\ntrain_ds = tf.data.Dataset.zip((train_img_ds, train_cap_ds)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nval_ds = tf.data.Dataset.zip((val_img_ds, val_cap_ds)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_ds = tf.data.Dataset.zip((test_img_ds, test_cap_ds)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:28.404111Z","iopub.execute_input":"2022-04-28T23:31:28.404787Z","iopub.status.idle":"2022-04-28T23:31:28.423421Z","shell.execute_reply.started":"2022-04-28T23:31:28.40475Z","shell.execute_reply":"2022-04-28T23:31:28.422801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sanity check for the division of datasets","metadata":{}},{"cell_type":"code","source":"# Some global variables\nEMBEDDIN_DIM = 512\nVOCAB_SIZE = 10000\nUNITS_RNN = 256","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:28.424751Z","iopub.execute_input":"2022-04-28T23:31:28.424994Z","iopub.status.idle":"2022-04-28T23:31:28.429052Z","shell.execute_reply.started":"2022-04-28T23:31:28.424962Z","shell.execute_reply":"2022-04-28T23:31:28.428185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    \n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        self.embedding_dim = embedding_dim\n        \n    def build(self, input_shape):\n        self.resnet = tf.keras.applications.ResNet50(include_top=False,\n                                                     weights='imagenet')\n        self.resnet.trainable=False\n        self.gap = GlobalAveragePooling2D()\n        self.fc = Dense(units=self.embedding_dim,\n                        activation='sigmoid')\n        \n    def call(self, x):\n        x = self.resnet(x)\n        x = self.gap(x)\n        x = self.fc(x)\n        return x\n\n    # Checking the CNN\nencoder = CNN_Encoder(EMBEDDIN_DIM)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:28.430312Z","iopub.execute_input":"2022-04-28T23:31:28.431054Z","iopub.status.idle":"2022-04-28T23:31:28.458245Z","shell.execute_reply.started":"2022-04-28T23:31:28.431017Z","shell.execute_reply":"2022-04-28T23:31:28.457589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n    def __init__(self, embedding_dim, units, vocab_size):\n        super(RNN_Decoder, self).__init__()\n        self.units = units\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        self.embedding = Embedding(input_dim=self.vocab_size,\n                                   output_dim=self.embedding_dim)\n    \n    def build(self, input_shape):\n        self.gru1 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.gru2 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.gru3 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.gru4 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.fc1 = Dense(self.units)\n        self.fc2 = Dense(self.vocab_size)\n\n    def call(self, x, initial_zero=False):\n        # x, (batch, 512)\n        # hidden, (batch, 256)\n        if initial_zero:\n            initial_state = decoder.reset_state(batch_size=x.shape[0])\n            output, state = self.gru1(inputs=x,\n                                      initial_state=initial_state)\n            output, state = self.gru2(inputs=output,\n                                      initial_state=initial_state)\n            output, state = self.gru3(inputs=output,\n                                      initial_state=initial_state)\n            output, state = self.gru4(inputs=output,\n                                      initial_state=initial_state)\n        else:\n            output, state = self.gru1(inputs=x)\n            output, state = self.gru2(inputs=output)\n            output, state = self.gru3(inputs=output)\n            output, state = self.gru4(inputs=output)\n        # output, (batch, 256)\n        x = self.fc1(output)\n        x = self.fc2(x)\n        \n        return x, state\n    \n    def embed(self, x):\n        return self.embedding(x)\n    \n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:28.46091Z","iopub.execute_input":"2022-04-28T23:31:28.46109Z","iopub.status.idle":"2022-04-28T23:31:28.475457Z","shell.execute_reply.started":"2022-04-28T23:31:28.461068Z","shell.execute_reply":"2022-04-28T23:31:28.474801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the RNN\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)\nfor image, caption in train_ds.take(1):\n    features = tf.expand_dims(encoder(image),1) # (batch, 1, 128)\n    em_words = decoder.embed(caption)\n    x = tf.concat([features,em_words],axis=1)\n    print(x.shape)\n    predictions, state = decoder(x, True)\n    print(predictions.shape)\n    print(state.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:28.476832Z","iopub.execute_input":"2022-04-28T23:31:28.477112Z","iopub.status.idle":"2022-04-28T23:31:42.23458Z","shell.execute_reply.started":"2022-04-28T23:31:28.477059Z","shell.execute_reply":"2022-04-28T23:31:42.23378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = CNN_Encoder(EMBEDDIN_DIM)\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:42.235978Z","iopub.execute_input":"2022-04-28T23:31:42.236416Z","iopub.status.idle":"2022-04-28T23:31:42.24947Z","shell.execute_reply.started":"2022-04-28T23:31:42.236375Z","shell.execute_reply":"2022-04-28T23:31:42.248732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use `Adam` as the optimizer.\n\nThe loss is `SparseCategoricalCrossentropy`, because here it would be inefficient to use one-hot-encoders are the ground truth. We will also use mask to help mask the `<pad>` so that we do not let the sequence model learn to overfit on the same.","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:42.250944Z","iopub.execute_input":"2022-04-28T23:31:42.251374Z","iopub.status.idle":"2022-04-28T23:31:42.258942Z","shell.execute_reply.started":"2022-04-28T23:31:42.251334Z","shell.execute_reply":"2022-04-28T23:31:42.257799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n    # img_tensor (batch, 224,224,3)\n    # target     (batch, 80)\n    loss = 0\n    with tf.GradientTape() as tape:\n        features = tf.expand_dims(encoder(img_tensor),1) # (batch, 1, 128)\n        em_words = decoder.embed(target)\n        x = tf.concat([features,em_words],axis=1)\n        predictions, _ = decoder(x, True)\n\n        loss = loss_function(target[:,1:], predictions[:,1:-1,:])\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:42.260497Z","iopub.execute_input":"2022-04-28T23:31:42.261015Z","iopub.status.idle":"2022-04-28T23:31:42.270363Z","shell.execute_reply.started":"2022-04-28T23:31:42.26098Z","shell.execute_reply":"2022-04-28T23:31:42.269521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef val_step(img_tensor, target):\n    # img_tensor (batch, 224,224,3)\n    # target     (batch, 80)\n    loss = 0\n    features = tf.expand_dims(encoder(img_tensor),1) # (batch, 1, 128)\n    em_words = decoder.embed(target)\n    x = tf.concat([features,em_words],axis=1)\n    predictions, _ = decoder(x, True)\n    loss = loss_function(target[:,1:], predictions[:,1:-1,:])\n    \n    loss = train_step(img_tensor, target)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:42.271695Z","iopub.execute_input":"2022-04-28T23:31:42.272065Z","iopub.status.idle":"2022-04-28T23:31:42.280416Z","shell.execute_reply.started":"2022-04-28T23:31:42.272031Z","shell.execute_reply":"2022-04-28T23:31:42.279295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nEPOCHS = 10\nepoch_wise_loss = []\nepoch_wise_val_loss = []\nfor epoch in range(EPOCHS):\n    batch_wise_loss = []\n    for (batch, (img_tensor, target)) in enumerate(train_ds):\n        loss = train_step(img_tensor, target)\n        batch_wise_loss.append(loss.numpy())\n        if batch%100 == 0:\n            print(f'Epoch: {epoch} Batch: {batch} Loss: {batch_wise_loss[-1]:.3f}')\n   \n    epoch_wise_loss.append(np.mean(batch_wise_loss))\n\n    batch_wise_val_loss = []\n    for (batch, (img_tensor, target)) in enumerate(val_ds):\n        loss = val_step(img_tensor, target)\n        batch_wise_val_loss.append(loss.numpy())\n    epoch_wise_val_loss.append(np.mean(batch_wise_val_loss))\n    print(f'Epoch: {epoch} Total Loss: {epoch_wise_loss[-1]:.3f} Val Loss:{epoch_wise_val_loss[-1]:.3f}')\n    print('-'*40)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T23:31:42.282224Z","iopub.execute_input":"2022-04-28T23:31:42.282552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_wise_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nepochs = 10\nfig, ax = plt.subplots(figsize=(8,7))\nplt.plot(range(epochs), epoch_wise_loss , color='r')\nplt.plot(range(epochs), epoch_wise_val_loss, color='b')\nplt.legend([\"Train Loss\", \"Validation Loss\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nax.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the weights","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.savefig('foo.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir models\nencoder.save_weights('./models/encoder.h5')\ndecoder.save_weights('./models/decoder.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n## Total Loss","metadata":{}},{"cell_type":"code","source":"encoder = CNN_Encoder(EMBEDDIN_DIM)\nfor image, caption in train_ds.take(1):\n    encoder(image)\n\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)\nfor image, caption in train_ds.take(1):\n    features = tf.expand_dims(encoder(image),1)\n    em_words = decoder.embed(caption)\n    x = tf.concat([features,em_words],axis=1)\n    predictions, state = decoder(x, True)\n\nencoder.load_weights('./models/encoder.h5')\ndecoder.load_weights('./models/decoder.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T00:31:26.307937Z","iopub.execute_input":"2022-04-27T00:31:26.308452Z","iopub.status.idle":"2022-04-27T00:31:35.034329Z","shell.execute_reply.started":"2022-04-27T00:31:26.308413Z","shell.execute_reply":"2022-04-27T00:31:35.033563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nfor img, cap in test_ds.take(1):\n   \n    img = tf.expand_dims(img[0],0)\n    cap = tf.expand_dims(cap[0],0)\n\n    feature = tf.expand_dims(encoder(img),1) # (1, 1, 128)\n    prediction, _ = decoder(feature, True)\n\n    word = tf.reshape(tokenizer.word_index['<start>'], shape=(1,1))\n    em_words = decoder.embed(word)\n\n    prediction, _ = decoder(em_words)\n    idx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\n    word = tokenizer.index_word[idx]\n\n    count = 0\n    catption = ''\n    while word != '<end>':\n        print(word, end=\" \")\n        caption += word\n        word_int = tf.reshape(tokenizer.word_index[word], shape=(1,1))  \n        em_words = decoder.embed(word_int)\n        prediction, _ = decoder(em_words)\n        idx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\n        word = tokenizer.index_word[idx]\n    \n    data.append(img, caption)\n        \ndf_test = pd.DataFrame(data)\nprint(\"test DF Created with captions\")","metadata":{"execution":{"iopub.status.busy":"2022-04-27T00:32:17.797756Z","iopub.execute_input":"2022-04-27T00:32:17.798053Z","iopub.status.idle":"2022-04-27T00:32:17.802316Z","shell.execute_reply.started":"2022-04-27T00:32:17.798017Z","shell.execute_reply":"2022-04-27T00:32:17.801502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.to_csv('df_test_caption.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T22:28:25.286421Z","iopub.execute_input":"2022-04-26T22:28:25.286941Z","iopub.status.idle":"2022-04-26T22:28:25.293158Z","shell.execute_reply.started":"2022-04-26T22:28:25.286898Z","shell.execute_reply":"2022-04-26T22:28:25.292266Z"},"trusted":true},"execution_count":null,"outputs":[]}]}